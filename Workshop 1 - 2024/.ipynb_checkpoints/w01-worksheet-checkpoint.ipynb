{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left; font-size:90%\">\n",
    "    BUSA90542 Machine Learning\n",
    "    <span style=\"float:right;\">Â© 2022 University of Melbourne (excluding links to external images)</span>\n",
    "</p>\n",
    "\n",
    "# Workshop 1: Evaluation and Naive Bayes\n",
    "***\n",
    "In this worksheet we cover:\n",
    "* [evaluation](#Evaluation)\n",
    "* [cross-validation](#Cross-validation)\n",
    "* [Naive Bayes classifiers](#Naive-Bayes-classifiers)\n",
    "\n",
    "Below we import the packages we need for the coding questions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation\n",
    "Evaluation is an important part of the machine learning workflow. Before deploying a machine learning model, it's essential to understand how it performs.\n",
    "\n",
    "### Sampling for evaluation\n",
    "It would be wrong to evaluate a model using data that has already been used for training. Doing so would lead to a _biased_ (often optimistic) estimate of the performance, as the model may have overfit the training data. To obtain an _unbiased_ estimate of the performance, we should use labelled data that's _independent_ of the training data. This allows us to assess whether the model generalises to new (but representative) data outside the training data.\n",
    "\n",
    "There are two practical evaluation methods:\n",
    "* **Holdout method:** a random subset of the labelled data set is reserved for evaluation. This is often called a test set or holdout set.\n",
    "* **Cross-validation:** evaluation is repeated multiple times for different random splits into train/test sets. There are a few variants:\n",
    " * **Random sub-sampling:** each train/test split is drawn with replacement from the original data set.\n",
    " * **k-fold cross-validation:** the original data set is partitioned into $k$ folds and the model is evaluated over $k$ rounds. In each round one fold is used as the test set (the remaining folds are used for training).\n",
    " * **Leave-one-out cross-validation:** the model is evaluated over $n$ rounds (where $n$ is the number of instances in the data set). In each round, one instance is \"left out\" (used for testing) and the remaining instances are used for training. \n",
    "\n",
    "We'll cover cross-validation in more detail shortly.\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "#### Question 1 (Discussion)\n",
    "Suppose Alice takes a labelled dataset $D$ with 100 instances and 1000 features. She computes the correlation of each feature with the class label and discards 950 features with the lowest correlation. She now has a processed version $D'$ of the dataset (with only 50 features). She splits $D'$ into a train/test set with an 70:30 split. She fits a linear regression model on the training set and evaluates the model accuracy on the test set. She reports the accuracy as 90\\%. Why might this estimate of 90\\% accuracy be overly optimistic? Give reasons.\n",
    "</blockquote>\n",
    "\n",
    "### Evaluation measures for binary classification\n",
    "A binary classifier is evaluated on a test set by comparing the classifier's predictions to the ground truth. For each test instance, there are only four possibilities:\n",
    "* true positive: instance is **predicted positive** and **truth is positive**\n",
    "* true negative: instance is **predicted negative** and **truth is negative**\n",
    "* false positive: instance is **predicted positive** but **truth is negative**\n",
    "* false negative: instance is **predicted negative** but **truth is positive**\n",
    "\n",
    "Errors are only made in the last two cases. We often count the number of true positives, true negatives, etc. and represent the counts in a table called a _confusion matrix_.\n",
    "\n",
    "<img src=\"confusion-matrix.png\"  style=\"width:300px;\">\n",
    "\n",
    "Evaluation measures distill information from the confusion matrix into a single number (often between 0 and 1). Some examples are listed below.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Error rate}  &= \\frac{\\mathrm{FP} + \\mathrm{FN}}{\\mathrm{TP} + \\mathrm{TN} + \\mathrm{FP} + \\mathrm{FN}} \\\\\n",
    "\\mathrm{Accuracy}  &= \\frac{\\mathrm{TP} + \\mathrm{TN}}{\\mathrm{TP} + \\mathrm{TN} + \\mathrm{FP} + \\mathrm{FN}} \\\\\n",
    "\\mathrm{Precision} &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} \\\\\n",
    "\\mathrm{Recall}    &= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\\n",
    "\\text{F1-score}    &= 2 \\times \\frac{\\mathrm{Precision} \\times \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #fef5e7;\">\n",
    "\n",
    "#### Question 2 (Pen/paper)\n",
    "Consider the task of building a classifier from random data, where the attribute values are generated randomly irrespective of the class labels. Assume the data set contains instances from two classes, $+$ and $-$. Half of the data set is used for training and the remaining half is used for testing.\n",
    "\n",
    "1. Suppose there are an equal number of $+$ and $-$ instances in the data. Consider a classifier that predicts $+$ for all test instances. What is the expected error rate of the classifier on the test data?\n",
    "\n",
    "1. Repeat the previous analysis assuming that the classifier predicts $+$ with probability 0.8 and $-$ with probability 0.2 (for any given test record).\n",
    "\n",
    "1. Suppose two-thirds of the data belongs to the $+$ class and the remaining one-third belongs to the $-$ class. What is the expected error of a classifier that predicts $+$ for every test instance?\n",
    "\n",
    "1. Repeat the previous analysis assuming that the classifier predicts predicts $+$ with probability $\\frac{2}{3}$ and $-$ with probability $\\frac{1}{3}$ (for any given test instance).\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver operating characteristic curve\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/36/Roc-draft-xkcd-style.svg\">\n",
    "\n",
    "The receiver operating characteristic (ROC) curve illustrates the performance of a binary classifier as its threshold is varied. It shows the trade-off between the two types of errors that can be made: false positives and false negatives. The _true positive rate_\n",
    "$$\n",
    "\\mathrm{TPR} = \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} = \\mathrm{Recall} = \\mathrm{Sensitivity}\n",
    "$$\n",
    "is plotted on the $y$-axis and the _false positive rate_\n",
    "$$\n",
    "\\mathrm{FPR} = \\frac{\\mathrm{FP}}{\\mathrm{FP} + \\mathrm{TN}} = \\text{Fall-out} = 1 - \\mathrm{Specificity}\n",
    "$$\n",
    "is plotted on the $x$-axis.\n",
    "\n",
    "**Example:** Suppose a classifier is used to identify people with a disease. Ideally the classifier should have a high TPR and a low FPR. It has a high TPR if it does a good job at detecting people with the disease (few false negatives). It has a low FPR if it rarely predicts healthy people are diseased (few false positives).\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #fef5e7;\">\n",
    "\n",
    "#### Question 3 (Pen/paper)\n",
    "You are asked to evaluate the performance of two classification models, $M_1$ and $M_2$. The test set you are given contains 26 binary attributes labeled \"A\" through \"Z\". The table below shows the posterior probabilities obtained by applying the models to the test set. (Only the posterior probabilities for the positive class are shown). As this is a two-class problem, $P(-) = 1 - P(+)$ and $P(-|A,\\ldots,Z) = 1 - P(+|A,\\ldots,Z)$. Assume that we are mostly interested in detecting instances from the positive class.\n",
    "\n",
    "Instance | True Class | $P(+|A,\\ldots,Z,M_1)$ | $P(+|A,\\ldots,Z,M_2)$\n",
    "--- | --- | --- | ---\n",
    "1  | $+$ | 0.73 | 0.61\n",
    "2  | $+$ | 0.69 | 0.03\n",
    "3  | $-$ | 0.44 | 0.68\n",
    "4  | $-$ | 0.55 | 0.31\n",
    "5  | $+$ | 0.67 | 0.45\n",
    "6  | $+$ | 0.47 | 0.09\n",
    "7  | $-$ | 0.08 | 0.38\n",
    "8  | $-$ | 0.15 | 0.05\n",
    "9  | $+$ | 0.45 | 0.01\n",
    "10 | $-$ | 0.35 | 0.04\n",
    "\n",
    "1. Plot the ROC curve for both $M_1$ and $M_2$ (You should plot them on the same graph.) Compute the AUC for $M_1$.  Compute the AUC for $M_2$. Which model do you think is better? Explain your reasons.\n",
    "\n",
    "1. For model $M_1$, suppose you choose the cutoff threshold to be $t = 0.5$. In other words, any test instances whose posterior probability is greater than $t$ will be classified as a positive example. Compute the precision, recall, and F-measure for the model at this threshold value.\n",
    "\n",
    "1. Repeat the analysis for part (b) using the same cutoff threshold on model $M_2$. Compare the F-measure results for both models. Which model is better? Are the results consistent with what you expect from the ROC curve?\n",
    "</blockquote>\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 4 (Coding)\n",
    "Repeat the question above in Python. For your convenience, the code block below loads the data from the table into three NumPy arrays: \n",
    "* `y` true class labels, \n",
    "* `scores_1` $P(+|A,\\ldots,Z,M_1)$\n",
    "* `scores_2` $P(+|A,\\ldots,Z,M_2)$\n",
    "\n",
    "Useful scikit-learn functions are \n",
    "* `sklearn.metrics.roc_curve`\n",
    "* `sklearn.metrics.auc`.\n",
    "* `sklearn.metrics.precision_score`\n",
    "* `sklearn.metrics.recall_score`\n",
    "* `sklearn.metrics.f1_score`\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table data into NumPy arrays\n",
    "table_str = \"\"\"\n",
    "id | y | scores_1 | scores_2\n",
    "1  | $+$ | 0.73 | 0.61\n",
    "2  | $+$ | 0.69 | 0.03\n",
    "3  | $-$ | 0.44 | 0.68\n",
    "4  | $-$ | 0.55 | 0.31\n",
    "5  | $+$ | 0.67 | 0.45\n",
    "6  | $+$ | 0.47 | 0.09\n",
    "7  | $-$ | 0.08 | 0.38\n",
    "8  | $-$ | 0.15 | 0.05\n",
    "9  | $+$ | 0.45 | 0.01\n",
    "10 | $-$ | 0.35 | 0.04\n",
    "\"\"\"\n",
    "\n",
    "table_df = pd.read_csv(pd.compat.StringIO(table_str), sep=\"\\s\\|\\s\", engine='python')\n",
    "y = table_df.y.map({'$+$': 1, '$-$': 0}).values\n",
    "scores_1 = table_df.scores_1.values\n",
    "scores_2 = table_df.scores_2.values\n",
    "\n",
    "print(\"Labels:\", y)\n",
    "print(\"Scores from model 1:\", scores_1)\n",
    "print(\"Scores from model 2:\", scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# Use help(roc_curve) and help(auc) to check the input and output of them\n",
    "\n",
    "\n",
    "# Compute the ROC curves\n",
    "\n",
    "fpr_1, tpr_1, thresholds_1 = roc_curve(y, scores_1, pos_label=1) # ROC curve for M1\n",
    "... # ROC curve for M2\n",
    "\n",
    "\n",
    "# Compute the AUC\n",
    "\n",
    "auc_1 = ... # AUC for M1\n",
    "... # AUC for M2\n",
    "\n",
    "\n",
    "# Make standard ROC plot\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr_1, tpr_1, color='b', label='Model 1 (AUC = %0.2f)' % auc_1)\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "plt.xlim([-0.03, 1.03])\n",
    "plt.ylim([-0.03, 1.03])\n",
    "\n",
    "\n",
    "###### Fiil in#######\n",
    "#Set title and label for the x and y axis\n",
    "# you can use help(plt) and search \"title\" to find the function you need\n",
    "\n",
    "\n",
    "\n",
    "###### Your code end here#######\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Fill in\n",
    "\n",
    "prec_m1 = ...\n",
    "rec_m1 = ...\n",
    "f1_m1 = ...\n",
    "prec_m2 = ...\n",
    "rec_m2 = ...\n",
    "f1_m2 = ...\n",
    "\n",
    "\n",
    "print(\"Model 1\")\n",
    "print(\"Precision:\", prec_m1)\n",
    "print(\"Recall:\", rec_m1)\n",
    "print(\"F1:\", f1_m1)\n",
    "\n",
    "print(\"-------\")\n",
    "\n",
    "print(\"Model 2\")\n",
    "print(\"Precision:\", prec_m2)\n",
    "print(\"Recall:\", rec_m2)\n",
    "print(\"F1:\", f1_m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Cross-validation\n",
    "Cross-validation is an evaluation method that can be used when data is scarce. Training and evaluation are repeated multiple times for different random splits of the data into train/test sets. The final evaluation measure is taken as an average over the rounds. By repeating the evaluation over multiple rounds, the variability of the estimate is reduced.\n",
    "\n",
    "### K-fold cross-validation\n",
    "Here we focus on $k$-fold cross-validation. The key steps are as follows:\n",
    "1. The data is partitioned into $k$ equal parts (folds)\n",
    "1. Training and evaluation are repeated for $k$ rounds. In each round, one fold is used for testing and the remaining $k-1$ folds are used for training.\n",
    "1. The evaluation measure is averaged over the $k$ rounds.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg\" style=\"width:450px;\">\n",
    "\n",
    "### [Aside] Parameter tuning/model selection\n",
    "Cross-validation can also be used to tune model parameters and/or to select the best model among some candidates. The motivation is the same as described above: we want to assess how the models generalise without using too much data. This is illustrated in the diagram below. Here we are choosing the best parameter setting among 5 different choices.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use $k$-fold cross-validation to estimate the accuracy of a classifier on the Iris flower data set. This data set contains observations of three species of Iris flowers. There are four features for each observation: the length and width of the sepals and petals in centimetres. The goal is to predict the species based on the measurements.\n",
    "\n",
    "In the code block below we load the data into a feature matrix `X` and label vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(\"There are {} instances and {} features\".format(X.shape[0], X.shape[1]))\n",
    "print(\"There are {} classes\".format(np.unique(y).size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how cross-validation works in scikit-learn. $k$-fold cross-validation is implemented in `sklearn.model_selection.KFold`. \n",
    "\n",
    "We start by instantiating a KFold instance, which takes the number of splits ($k$) as an argument. You can also specify whether to randomly shuffle the data before partitioning into folds (it's safest to set this to True).\n",
    "\n",
    "We then call the `split` method on the iris data set and carry out some operations for each round in a for-loop.\n",
    "Note that the `train` and `test` arrays for each round are indices into the original data. We can access the feature vectors/labels using array indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=2, shuffle=True)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"-----\")\n",
    "    print(\"Items in train:\", train)\n",
    "    print(\"Items in test:\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we estimate the accuracy of a logistic regression classifier using a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on held-out test set:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 5 (Coding)\n",
    "Estimate the accuracy of the logistic regression classifier using 10-fold cross validation. How does this compare to the estimate obtained using the hold-out method above?\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "#### Question 6 (Discussion)\n",
    "Later in the course we'll see that it's sometimes important to standardize the features so that they vary on roughly the same scale. A common approach is to subtract the mean and divide by the standard deviation (for each feature). If we wanted to standardize the features, does it make any difference whether this is done before or inside the cross-validation for-loop?\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Naive Bayes classifiers\n",
    "The Naive Bayes classifier is a probabilistic classifier that makes strong (naive) independence assumptions between the features. The probability distribution over the class label $y \\in \\{1, \\ldots, C\\}$ given a feature vector $\\mathbf{x} = [x_1, \\cdots, x_p]$ is assumed to have the following form:\n",
    "$$\n",
    "p(y = c| \\mathbf{x}) = \\frac{p(c) \\prod_{i = 1}^{p} p(x_i | c)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "This comes from applying Bayes' theorem and assuming $p(x_i | x_{i+1}, \\ldots, x_p, c)= p(x_i | c)$ holds true for all features $i$.\n",
    "\n",
    "When implementing a Naive Bayes classifier, one must make some assumptions about the form of the distributions. Common variants implemented in scikit-learn are:\n",
    "* `GaussianNB` assumes $p(x_i|c)$ corresponds to a Gaussian distribution (continuous data)\n",
    "* `MultinomialNB` assumes $p(\\mathbf{x}|c)$ corresponds to a Multinomial distribution (count data)\n",
    "* `BernoulliNB` assumes $p(x_i|c)$ corresponds to a Bernoulli distribution (binary data)\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #fef5e7;\">\n",
    "\n",
    "#### Question 7 (Pen/paper)\n",
    "Consider the following dataset\n",
    "\n",
    "Instance | A | B | C | Class\n",
    "--- | --- | --- | --- | ---\n",
    "1  | 0 | 0 | 0 | +\n",
    "2  | 0 | 0 | 1 | -\n",
    "3  | 0 | 1 | 1 | -\n",
    "4  | 0 | 1 | 1 | -\n",
    "5  | 0 | 0 | 1 | +\n",
    "6  | 1 | 0 | 1 | +\n",
    "7  | 1 | 0 | 1 | -\n",
    "8  | 1 | 0 | 1 | -\n",
    "9  | 1 | 1 | 1 | +\n",
    "10 | 1 | 0 | 1 | +\n",
    "\n",
    "1. Estimate the conditional probabilities for $P(A|+)$, $P(B|+)$, $P(C|+)$, $P(A|-)$, $P(B|-)$, and $P(C|-)$.\n",
    "\n",
    "2. Use the estimate of conditional probabilities given in the previous question to predict the class label for a test instance $(A = 0,B = 1,C = 0)$ using the naive Bayes approach.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
