{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:left; font-size:90%\">\n",
    "    MBUSA Machine Learning\n",
    "    <span style=\"float:right;\">© 2024 University of Melbourne (excluding links to external images)</span>\n",
    "</p>\n",
    "\n",
    "# Workshop 2: Decision Trees & Ensemble Methods\n",
    "***\n",
    "In this worksheet we cover:\n",
    "* [Decision trees](#2a:-Decision-Trees)\n",
    "* [Ensemble methods: bagging and boosting](#Workshop-2b:-Ensemble-Methods,-bagging-and-boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the packages we need for the coding questions.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2a: Decision Trees\n",
    "\n",
    "### Entropy\n",
    "Entropy is a measure of the _average information_ produced by a random process, or contained within a random sample. It can also be viewed as a measure of _diversity_ or _purity_.\n",
    "\n",
    "Given a categorical random variable, the information entropy is defined as:\n",
    "$$\n",
    "H(X) = - \\sum_{x} p(x) \\log_2 p(x)\n",
    "$$\n",
    "where $p(x)$ is the probability mass associated with category $x$.\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #fef5e7;\">\n",
    "\n",
    "#### Quick Question 1 (Pen/paper)\n",
    "***Entropy***\n",
    "\n",
    "Consider repeatedly tossing a coin. Calculate the entropy per toss in the following cases:\n",
    "\n",
    "1. The coin is fair (probability of heads/tails is $0.5$).\n",
    "2. The coin is biased so that the probability of heads is $0.7$ and the probability of tails is $0.3$.\n",
    "3. The coin only returns heads (probability of tails is $0$).\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "Information gain is a commonly used metric for assessing the quality of splits in decision trees. It's used in the ID3, C4.5 and C5 decision tree algorithms.\n",
    "\n",
    "Information gain is closely linked to information entropy. It measures the amount of information _gained_ about a random variable $Y$ after observing another random variable $X$. It's defined as follows:\n",
    "$$\n",
    "IG(Y,X) = H(Y) - H(Y|X)\n",
    "$$\n",
    "The second term is the conditional entropy. Assuming $X$ is discrete, it can be expressed as follows:\n",
    "$$\n",
    "H(Y|X) = \\sum_{x} p(x) H(Y\\vert X=x)\n",
    "$$\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #fef5e7;\">\n",
    "    \n",
    "#### Quick Question 2 (Pen/paper)\n",
    "***Information gain***\n",
    "\n",
    "Consider the following binary classification data set. We'd like to determine a good splitting rule for the first node in the tree. Note that features A and B are binary and feature C is continuous.\n",
    "\n",
    "Feature A | Feature B | Feature C | Class Label\n",
    "--- | --- | --- | ---\n",
    "T | F | 6 | +\n",
    "T | T | 7 | +\n",
    "T | T | 8 | +\n",
    "T | F | 6.5 | -\n",
    "T | T | 5 | +\n",
    "F | F | 5.5 | -\n",
    "F | F | 4 | -\n",
    "F | F | 4.5 | -\n",
    "T | T | 3 | -\n",
    "T | F | 2 | -\n",
    "\n",
    "1. Compute the information gain when splitting on feature A.\n",
    "2. Compute the information gain when splitting on feature B.\n",
    "3. Of features A and B, which produces the best split?\n",
    "4. What if we wanted to consider feature C for splitting? How could we compute the information gain?\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees\n",
    "In lectures, we covered the Iterative Dichotomiser 3 (ID3) algorithm for fitting a decision tree.\n",
    "The default model in sklearn is Classification and Regression Trees (CART), that uses Gini as the split criterion.\n",
    "If you want to use ID3, you just have to set the attribute criterion: \"entropy\", instead of the default value, which is Gini. \n",
    "\n",
    "The table below summarises how the three most popular decision tree algorithms differ.\n",
    "\n",
    " Algorithm | Splitting criterion | Supported attribute types | Supports missing values | Pruning strategy | Outlier detection \n",
    " --- | --- | --- | --- | --- | ---\n",
    "ID3  | Information gain | Categorical | No | None | Susceptible to outliers\n",
    "CART | Gini or twoing | Categorical and numeric | Yes | Cost complexity pruning | Handles outliers\n",
    "C4.5 | Gain ratio | Categorical and numeric | Yes | Error based pruning | Susceptible to outliers\n",
    "\n",
    "Let's fit a decision tree using scikit-learn.\n",
    "But first we'll load some data from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php).\n",
    "The data set we'll be using contains records for 45,211 telemarketing customer interactions at a Portuguese bank. \n",
    "The goal is to predict whether the customer will sign up for the product—i.e. a binary classification task. \n",
    "There are 16 mixed (categorical/numeric) features for each customer interaction. \n",
    "\n",
    "The code block below downloads the data from the repository, unzips it, and reads the full CSV file into a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
       "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown  no  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "resp = urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip')\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "# !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\n",
    "# zipfile = ZipFile(\"bank.zip\")\n",
    "\n",
    "df = pd.read_csv(zipfile.open('bank-full.csv'), header=0, sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that several features are categorical. We must therefore apply feature encoding to encode categorical features as arrays. A straightforward solution is to apply the pandas.get_dummies function to one-hot encode all of the categorical columns in the DataFrame. We then cast the result to a NumPy array called X. Notice that we exclude the labels, which are stored in column y.\n",
    "\n",
    "We also re-encode the labels—0 for 'no' and 1 for 'yes'—and cast to a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed = pd.get_dummies(df.drop('y', axis=1))\n",
    "X = df_transformed.values\n",
    "Y = df.y.map({'no':0, 'yes':1}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the decision tree, let's perform warm-up exercises to learn how to use sklearn for training, prediction and evaluation. We will train a basline model with Majority Class, which is the commonly used baseline in ML. \n",
    "\n",
    "The following are three useful functions, notice they are not only for the Majority class, but also for most sklearn training alrogithms (e.g. Decision Tree, Random Forest, etc.).\n",
    "\n",
    "&emsp;&emsp;&emsp;&emsp;`fit`: train model from the training set.\\\n",
    "&emsp;&emsp;&emsp;&emsp;`predict`: makde prediction for the input samples.\\\n",
    "&emsp;&emsp;&emsp;&emsp;`score`: return the evaluate result on the given test data and labels. (e.g mean accuracy for classification)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : [0 0 0 0 0 0 0 0 0 0]\n",
      "Accuracy: 0.8830151954170445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "ds_clf = DummyClassifier(strategy=\"most_frequent\") # Define our model, set parameter strategy to 'most_frequent'\n",
    "ds_clf.fit(X, Y) # Use model.fit to train with our dataset \n",
    "Y_predict = ds_clf.predict(X) # Use model.predict to make prediction\n",
    "print(\"Prediction :\", Y_predict[:10])\n",
    "print(\"Accuracy:\", ds_clf.score(X, Y)) # Use model.score to evaluate our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we got an accuracy of 0.88. But does this mean that our model is very powerful? Is the evaluation result reliable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 3 (Coding)\n",
    "Fit a decision tree to this data set using `sklearn.tree.DecisionTreeClassifier`.\n",
    "You should partition your data into train/test with a 70:30 ratio.\n",
    "Evaluate the tree on the test set using an appropriate measure.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable ellipsis object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Heldout - split dataset into training set and test set\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 4\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable ellipsis object"
     ]
    }
   ],
   "source": [
    "# Heldout - split dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = ... #fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-traini Majority class with heldout\n",
    "\n",
    "ds_clf.fit(...) # fill in\n",
    "Y_predict = ds_clf.predict(...) # fill in\n",
    "print(\"Prediction :\", Y_predict[:10])\n",
    "print(\"Accuracy on the test set:\", ds_clf.score(...)) # fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree model\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the tree\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plot_tree() # fill in\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "    \n",
    "#### Question 4 (Discussion)\n",
    "What hyperparameters did you choose for the decision tree? Will using different values affect the model results? \n",
    "    \n",
    "We mention pruning during the lecture. Could you find which parameter is for pruning in `sklearn.ensemble.RandomForestClassifier`. \n",
    "</blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Workshop 2b: Random Forest\n",
    "\n",
    "The pseudo-code for the Random Forest training algorithm is as follows:\n",
    "\n",
    "~~~\n",
    "1  Parameters: n_trees (number of trees); max_features (size of random feature subset)\n",
    "2  Initialise empty forest.\n",
    "3  While size of forest is less than n_trees:\n",
    "4      Create a bootstrap sample from the training data.\n",
    "5      Train a decision tree on the sample (with max_features randomly-selected features available for each split).\n",
    "6      Add tree to forest.\n",
    "~~~\n",
    "\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "    \n",
    "#### Question 5 (Coding)\n",
    "Fit a random forest model on the bank data using `sklearn.ensemble.RandomForestClassifier`. \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 6 (Code)\n",
    "Experiment with the hyperparameters for the random forest classifier, such as:\n",
    "- max_depth\n",
    "- max_features\n",
    "- max_leaf_nodes\n",
    "- ...\n",
    "\n",
    "Do they have an effect on the performance?\n",
    "How would you tune these parameters to avoid overfitting?\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, we test this statement empirically by varying the number of trees in the forest.\n",
    "Note the effect on test accuracy and running time. You can also try to tune other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_of_trees in [5, 10, 50, 100, 200]:\n",
    "    print(\"===============================\")\n",
    "    time_1 = time.time()\n",
    "    print('Set num_of_trees to {}'.format( num_of_trees ))\n",
    "    rf_clf = ... # fill in\n",
    "    print('Fit finished in {:.3g}s'.format(time.time() - time_1))\n",
    "    print('The test accuracy is {:.3g}'.format(...)) # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension questions\n",
    "\n",
    "In this section, we're now going to implement a Random Forest from scratch using `sklearn.tree.DecisionTreeClassifier` as the base learner.\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 7 (Coding)\n",
    "The following code block defines a `fit_forest` function, which takes in some training data and returns an ensemble of fitted decision trees.\n",
    "Complete the missing parts with reference to the pseudo-code above. The results should be nearly identical to the ones you obtained using sklearn.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest(X, Y, n_trees, **kwargs):\n",
    "    \"\"\"\n",
    "    Fit a Random Forest\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    Y : numpy array, shape (n_instances,)\n",
    "    n_trees : int\n",
    "        number of trees in the forest\n",
    "    *kwargs : keyword arguments passed to DecisionTreeClassifier\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    a dictionary with two entries:\n",
    "      'forest' : list of DecisionTreeClassifier instances\n",
    "      'oob_score' : the out-of-bag accuracy\n",
    "    \"\"\"\n",
    "    forest = []\n",
    "    n_instances = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    \n",
    "    # A matrix to store the out-of-bag predictions for each tree (in-bag are left as 'nan')\n",
    "    oob_all_predictions = np.full([n_trees, n_instances], np.nan)\n",
    "    \n",
    "    for i in range(n_trees):\n",
    "        bag_ids =  # fill in\n",
    "        tree =  # fill in\n",
    "        \n",
    "        # Save predictions for out-of-bag instances\n",
    "        oob_ids = np.setdiff1d(np.arange(n_instances), bag_ids)\n",
    "        oob_all_predictions[i,oob_ids] = tree.predict(X[oob_ids])\n",
    "        \n",
    "        forest.append(tree) \n",
    "    \n",
    "    # Compute the out-of-bag accuracy (majority vote)\n",
    "    from scipy.stats import mode\n",
    "    oob_vote_prediction = mode(oob_all_predictions, axis=0, nan_policy='omit')[0].flatten()\n",
    "    oob_score = (oob_vote_prediction==Y).mean()\n",
    "    \n",
    "    return {'forest': forest, 'oob_score': oob_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, this version is not strictly Random Foreset, because it's not doing the feature randomisation. It's only uses Bagging to a set of DecisionTrees.\n",
    "\n",
    "Let's apply this function to fit a Random Forest of 100 trees to the bank data. \n",
    "Note that a larger forest is generally better, but performance will diminish beyond a certain size.\n",
    "By setting `max_features='sqrt'`, we're restricting each split to a random subset of features of size $\\sqrt{\\mathrm{num\\ features}}$ (i.e. a subset of 8 features for this data).\n",
    "\n",
    "We also print out the out-of-bag accuracy, which we computed while fitting the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = fit_forest(X_train, Y_train, 10, max_features='sqrt')\n",
    "print('Out-of-bag accuracy is {:.3g}'.format(forest['oob_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #eafaf1;\">\n",
    "\n",
    "#### Question 8 (Coding)\n",
    "Complete the `predict_forest` function below to make predictions for a set of instances `X` using the forest you just trained. Remember that for classification, a prediction is made by majority vote among trees in the forest.\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_forest(X, forest):\n",
    "    \"\"\"\n",
    "    Make predictions using majority voting over the trees in the forest\n",
    "    \n",
    "    Arguments\n",
    "    =========\n",
    "    X : numpy array, shape: (n_instances, n_features)\n",
    "        feature matrix\n",
    "    forest : a list of DecisionTreeClassifier instances\n",
    "    \"\"\"\n",
    "    # Allocate array to store predictions for each instance/tree\n",
    "    n_instances = X.shape[0]\n",
    "    n_trees = len(forest)\n",
    "    predictions = np.empty([n_trees, n_instances], dtype=int)\n",
    "    \n",
    "    # Fill array\n",
    "    for (i,tree) in enumerate(forest):\n",
    "        predictions[i] =  # fill in\n",
    "    \n",
    "    # Use majority vote (mode) for each instance\n",
    "    from scipy.stats import mode\n",
    "    return mode(predictions, axis=0)[0].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `predict_forest` function to compute the accuracy on the test set. You should compare the test accuracy to the out-of-bag accuracy computed previously and to the test accuracy for the decision tree (from the previous section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = predict_forest(X_test, forest['forest'])\n",
    "\n",
    "test_acc =  # fill in\n",
    "print('The test accuracy is {:.3g}'.format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
