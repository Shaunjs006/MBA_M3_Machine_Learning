{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the workshop is dedicated to Support Vector Machines. The aims of this part are (i) to appreciate how the use of kernels results in non-linear decision boundaries, and (ii) to see how regularisation and kernel parameters affect this shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with same old initialisation line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cats Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are going to use a real-world dataset that is not linearly separable. The dataset consists of about 150 trainig examples with two attributes each. Each example corresponds to a cat and the two attributes are some physiological measurements for that cat. Along with the measuremnets, the sex of the cat is also recorded as a label. We encode sex using 1 for female cats and -1 for male cats.\n",
    "\n",
    "*(This data was popularised by the following publication: Fisher, R.A. (1947) The analysis of covariance method for the relation between a part and the whole. Biometrics, 3, 65â€“68.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cats_data():\n",
    "    d = numpy.array([\n",
    "[2.0, 7.0, 1],\n",
    "[2.0, 7.4, 1],\n",
    "[2.0, 9.5, 1],\n",
    "[2.1, 7.2, 1],\n",
    "[2.1, 7.3, 1],\n",
    "[2.1, 7.6, 1],\n",
    "[2.1, 8.1, 1],\n",
    "[2.1, 8.2, 1],\n",
    "[2.1, 8.3, 1],\n",
    "[2.1, 8.5, 1],\n",
    "[2.1, 8.7, 1],\n",
    "[2.1, 9.8, 1],\n",
    "[2.2, 7.1, 1],\n",
    "[2.2, 8.7, 1],\n",
    "[2.2, 9.1, 1],\n",
    "[2.2, 9.7, 1],\n",
    "[2.2, 10.9, 1],\n",
    "[2.2, 11.0, 1],\n",
    "[2.3, 7.3, 1],\n",
    "[2.3, 7.9, 1],\n",
    "[2.3, 8.4, 1],\n",
    "[2.3, 9.0, 1],\n",
    "[2.3, 9.0, 1],\n",
    "[2.3, 9.5, 1],\n",
    "[2.3, 9.6, 1],\n",
    "[2.3, 9.7, 1],\n",
    "[2.3, 10.1, 1],\n",
    "[2.3, 10.1, 1],\n",
    "[2.3, 10.6, 1],\n",
    "[2.3, 11.2, 1],\n",
    "[2.4, 6.3, 1],\n",
    "[2.4, 8.7, 1],\n",
    "[2.4, 8.8, 1],\n",
    "[2.4, 10.2, 1],\n",
    "[2.5, 9.0, 1],\n",
    "[2.5, 10.9, 1],\n",
    "[2.6, 8.7, 1],\n",
    "[2.6, 10.1, 1],\n",
    "[2.6, 10.1, 1],\n",
    "[2.7, 8.5, 1],\n",
    "[2.7, 10.2, 1],\n",
    "[2.7, 10.8, 1],\n",
    "[2.9, 9.9, 1],\n",
    "[2.9, 10.1, 1],\n",
    "[2.9, 10.1, 1],\n",
    "[3.0, 10.6, 1],\n",
    "[3.0, 13.0, 1],\n",
    "[2.0, 6.5, -1],\n",
    "[2.0, 6.5, -1],\n",
    "[2.1, 10.1, -1],\n",
    "[2.2, 7.2, -1],\n",
    "[2.2, 7.6, -1],\n",
    "[2.2, 7.9, -1],\n",
    "[2.2, 8.5, -1],\n",
    "[2.2, 9.1, -1],\n",
    "[2.2, 9.6, -1],\n",
    "[2.2, 9.6, -1],\n",
    "[2.2, 10.7, -1],\n",
    "[2.3, 9.6, -1],\n",
    "[2.4, 7.3, -1],\n",
    "[2.4, 7.9, -1],\n",
    "[2.4, 7.9, -1],\n",
    "[2.4, 9.1, -1],\n",
    "[2.4, 9.3, -1],\n",
    "[2.5, 7.9, -1],\n",
    "[2.5, 8.6, -1],\n",
    "[2.5, 8.8, -1],\n",
    "[2.5, 8.8, -1],\n",
    "[2.5, 9.3, -1],\n",
    "[2.5, 11.0, -1],\n",
    "[2.5, 12.7, -1],\n",
    "[2.5, 12.7, -1],\n",
    "[2.6, 7.7, -1],\n",
    "[2.6, 8.3, -1],\n",
    "[2.6, 9.4, -1],\n",
    "[2.6, 9.4, -1],\n",
    "[2.6, 10.5, -1],\n",
    "[2.6, 11.5, -1],\n",
    "[2.7, 8.0, -1],\n",
    "[2.7, 9.0, -1],\n",
    "[2.7, 9.6, -1],\n",
    "[2.7, 9.6, -1],\n",
    "[2.7, 9.8, -1],\n",
    "[2.7, 10.4, -1],\n",
    "[2.7, 11.1, -1],\n",
    "[2.7, 12.0, -1],\n",
    "[2.7, 12.5, -1],\n",
    "[2.8, 9.1, -1],\n",
    "[2.8, 10.0, -1],\n",
    "[2.8, 10.2, -1],\n",
    "[2.8, 11.4, -1],\n",
    "[2.8, 12.0, -1],\n",
    "[2.8, 13.3, -1],\n",
    "[2.8, 13.5, -1],\n",
    "[2.9, 9.4, -1],\n",
    "[2.9, 10.1, -1],\n",
    "[2.9, 10.6, -1],\n",
    "[2.9, 11.3, -1],\n",
    "[2.9, 11.8, -1],\n",
    "[3.0, 10.0, -1],\n",
    "[3.0, 10.4, -1],\n",
    "[3.0, 10.6, -1],\n",
    "[3.0, 11.6, -1],\n",
    "[3.0, 12.2, -1],\n",
    "[3.0, 12.4, -1],\n",
    "[3.0, 12.7, -1],\n",
    "[3.0, 13.3, -1],\n",
    "[3.0, 13.8, -1],\n",
    "[3.1, 9.9, -1],\n",
    "[3.1, 11.5, -1],\n",
    "[3.1, 12.1, -1],\n",
    "[3.1, 12.5, -1],\n",
    "[3.1, 13.0, -1],\n",
    "[3.1, 14.3, -1],\n",
    "[3.2, 11.6, -1],\n",
    "[3.2, 11.9, -1],\n",
    "[3.2, 12.3, -1],\n",
    "[3.2, 13.0, -1],\n",
    "[3.2, 13.5, -1],\n",
    "[3.2, 13.6, -1],\n",
    "[3.3, 11.5, -1],\n",
    "[3.3, 12.0, -1],\n",
    "[3.3, 14.1, -1],\n",
    "[3.3, 14.9, -1],\n",
    "[3.3, 15.4, -1],\n",
    "[3.4, 11.2, -1],\n",
    "[3.4, 12.2, -1],\n",
    "[3.4, 12.4, -1],\n",
    "[3.4, 12.8, -1],\n",
    "[3.4, 14.4, -1],\n",
    "[3.5, 11.7, -1],\n",
    "[3.5, 12.9, -1],\n",
    "[3.5, 15.6, -1],\n",
    "[3.5, 15.7, -1],\n",
    "[3.5, 17.2, -1],\n",
    "[3.6, 11.8, -1],\n",
    "[3.6, 13.3, -1],\n",
    "[3.6, 14.8, -1],\n",
    "[3.6, 15.0, -1],\n",
    "[3.7, 11.0, -1],\n",
    "[3.8, 14.8, -1],\n",
    "[3.8, 16.8, -1],\n",
    "[3.9, 14.4, -1],\n",
    "[3.9, 20.5, -1]])\n",
    "    \n",
    "    X = d[:,[0,1]]\n",
    "    y = d[:,2]\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the data looks like (black squares are male cats, red circles are female cats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = load_cats_data()\n",
    "plt.plot(X[y==-1,0], X[y==-1,1], \"ks\")\n",
    "plt.plot(X[y==1,0], X[y==1,1], \"ro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define an auxiliary function for plotting decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X are training data instances, y are labels\n",
    "# cls is a trained SVM classifier,\n",
    "# and name is the title of the plot\n",
    "def plot_boundary(X, y, cls, name):\n",
    "    # create a mesh to plot in\n",
    "    h = 0.02 # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(arange(x_min, x_max, h),\n",
    "                        arange(y_min, y_max, h))\n",
    "    \n",
    "    # plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    Z = cls.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    contourf(xx, yy, Z, cmap=cm.Paired, alpha=0.8)\n",
    "    \n",
    "    # plot also the training points\n",
    "    scatter(X[:, 0], X[:, 1], c=y, cmap=cm.Paired)\n",
    "    xlim(xx.min(), xx.max())\n",
    "    ylim(yy.min(), yy.max())\n",
    "    title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following excersises, you will be asked to experiment with various parameters. In all cases, you should pay attention to how these parameters control the degree of overfitting and try to explain the observed effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train soft margin SVM classifier with a linear kernel, which means that the original attributes are not transformed. Note that what is called $C$ in this implementation corresponds to $\\frac{C}{n}$ in lectures, where $n$ is the number of training instances. Also note that SVMs are called Support Vector Classifiers (SVCs) in this package. *Experiment with different values of $C$ and see how it affects the result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###answer to question 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work well, because the data is not linearly separable. Now try a polynomial kernel to see if the shape of the decision boundary can be changed. *Experiment with different polynomial degrees.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###answer to question 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore the Gaussian kernel (aka RBF kernel). Note that here $\\gamma=\\frac{1}{2\\sigma^2}$, where $\\sigma$ is the standard deviation of the Gaussian. *Experiment with different standard deviations of the kernel.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##answer to question 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Which kernel do you think works best for this data and why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer to question 4 here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
